#!/usr/bin/env python

import os
import sys
import argparse
import openai
import tiktoken
from gitignore_parser import parse_gitignore
import time
import json

def read_file_in_chunks(file_path, chunk_size):
    with open(file_path, 'r') as file:
        while True:
            lines = []
            for _ in range(chunk_size):
                line = file.readline()
                if not line:
                    break
                lines.append(line)
            if not lines:
                break
            yield ''.join(lines).strip()

def is_file_ignored_by_gitignore(file_path, gitignore_matchers):
    return any(matcher(file_path) for matcher in gitignore_matchers)

def load_gitignore_files(directory):
    gitignore_matchers = []
    current_dir = directory
    while current_dir != os.path.dirname(current_dir):  # Stop at the root directory
        gitignore_path = os.path.join(current_dir, '.gitignore')
        if os.path.exists(gitignore_path):
            gitignore_matchers.append(parse_gitignore(gitignore_path))
        current_dir = os.path.dirname(current_dir)

    # Check the current working directory's .gitignore
    cwd_gitignore_path = os.path.join(os.getcwd(), '.gitignore')
    if (os.path.exists(cwd_gitignore_path)):
        gitignore_matchers.append(parse_gitignore(cwd_gitignore_path))

    return gitignore_matchers

def call_openai_api(client, model, prompt, system_message=None, verbose=False):
    messages = []
    if system_message:
        messages.append({"role": "system", "content": system_message})
    messages.append({"role": "user", "content": prompt})

    if verbose:
        print(f"Raw request:\nModel: {model}\nMessages: {json.dumps(messages, indent=2)}\n", file=sys.stderr)

    start_time = time.time()
    response = client.chat.completions.create(
        model=model,
        messages=messages
    )
    elapsed_time = time.time() - start_time

    if verbose:
        print(f"Raw response:\n\t{str(response)}\n----------------\n\n", file=sys.stderr)

    return response.choices[0].message.content, elapsed_time

def count_tokens(text, encoder):
    return len(encoder.encode(text))

def process_files(directory, prompt, summary_prompt, context_length, model, system_message, file_filter, stats, verbose):
    print(f"directory is {directory}")
    print(f"prompt is {prompt}")
    print(f"summary_prompt is {summary_prompt}")
    print(f"context_length is {context_length}")
    print(f"model is {model}")
    print(f"system_message is {system_message}")
    print(f"file_filter is {file_filter}")
    print(f"stats is {stats}")
    print(f"verbose is {verbose}")
    encoder = tiktoken.encoding_for_model(model)
    total_responses = []
    total_input_tokens = 0
    total_output_tokens = 0
    total_api_time = 0

    client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))

    gitignore_matchers = load_gitignore_files(directory)

    for root, _, files in os.walk(directory):
        for file in files:
            if file.endswith(('.py', '.dockerfile', '.sh', '.js')):
                if file_filter and file_filter not in os.path.join(root, file):
                    continue
                file_path = os.path.join(root, file)
                if not is_file_ignored_by_gitignore(file_path, gitignore_matchers):
                    start_line = 1
                    chunk = ""
                    for line in read_file_in_chunks(file_path, 100):  # Read in chunks of 100 lines
                        chunk += line + "\n"
                        token_count = count_tokens(chunk, encoder)
                        if verbose:
                            print(f"Processing {file_path}, start_line {start_line}, token_count {token_count}", file=sys.stderr)
                        if token_count > context_length:
                            # Find the last whitespace before the context length boundary
                            split_point = chunk[:context_length].rfind(' ')
                            if split_point == -1:
                                split_point = context_length
                            chunk_to_send = chunk[:split_point]
                            remaining = chunk[split_point:].strip()
                            response, elapsed_time = call_openai_api(client, model, prompt.format(filename=file_path, startline=start_line, context=chunk_to_send), system_message, verbose)
                            total_responses.append(response)
                            total_input_tokens += count_tokens(chunk_to_send, encoder)
                            total_output_tokens += count_tokens(response, encoder)
                            total_api_time += elapsed_time
                            print(response)
                            start_line += chunk_to_send.count('\n')
                            chunk = remaining
                    if chunk:  # Process any remaining chunk
                        response, elapsed_time = call_openai_api(client, model, prompt.format(filename=file_path, startline=start_line, context=chunk), system_message, verbose)
                        total_responses.append(response)
                        total_input_tokens += count_tokens(chunk, encoder)
                        total_output_tokens += count_tokens(response, encoder)
                        total_api_time += elapsed_time
                        print(response)

    if summary_prompt:
        final_response, elapsed_time = call_openai_api(client, model, summary_prompt.format(context=' '.join(total_responses)), system_message, verbose)
        total_api_time += elapsed_time
        print(final_response)
        total_output_tokens += count_tokens(final_response, encoder)

    if stats:
        print("\n--- Stats ---", file=sys.stderr)
        print(f"Total execution time (API calls): {total_api_time:.2f} seconds", file=sys.stderr)
        print(f"Total input tokens: {total_input_tokens}", file=sys.stderr)
        print(f"Total output tokens: {total_output_tokens}", file=sys.stderr)
        print(f"Input tokens/sec: {total_input_tokens / total_api_time:.2f}", file=sys.stderr)
        print(f"Output tokens/sec: {total_output_tokens / total_api_time:.2f}", file=sys.stderr)

def main():
    parser = argparse.ArgumentParser(description="Process files with OpenAI API.")
    parser.add_argument('directory', help='Directory to process')
    parser.add_argument('-p', '--prompt', required=True, help='User prompt')
    parser.add_argument('-s', '--summary', help='Summary prompt')
    parser.add_argument('-c', '--context', type=int, required=True, help='Context length in tokens')
    parser.add_argument('-m', '--model', default='gpt-4o', help='Model name')
    parser.add_argument('--system', help='System message')
    parser.add_argument('-f', '--filter', help='Filter files by string in path')
    parser.add_argument('--stats', action='store_true', help='Print statistics')
    parser.add_argument('-v', '--verbose', action='store_true', help='Print raw request and response to stderr')

    args = parser.parse_args()
    print(f"args is {args}")

    process_files(
        directory=args.directory,
        prompt=args.prompt,
        summary_prompt=args.summary,
        context_length=args.context,
        model=args.model,
        system_message=args.system,
        file_filter=args.filter,
        stats=args.stats,
        verbose=args.verbose
    )
if __name__ == "__main__":
    main()
