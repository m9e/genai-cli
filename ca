#!/usr/bin/env python

# ca: (c)ommand-line (a)nalyzer
# example:
# ca -p 'You are a coding assistant. You are reading 10s of thousands of lines of code. For each chunk of code you will identify any class, method, function, and you will rate it on a scale of 1-3 where 1 is perfect use of type declarations; 2 means types are used but Any is used; 3 means types are used but there are some things missing types; 4 means many/all things are missing typing. After each chunk of code you will output ONLY the score first, then a string of filename:classname:method or filename:function, the starting line of the code chunk if we supply it, and up to 60 characters of explanation about why that rating. Do not output code or more than the 60 characters. Do not output more than 1 line and the newline for each method or function. Each output (for each method or function) should be exactly that one line of code with no linebreaks until after your 60 characters. This is filename {filename} and starts at line {startline}. Here is the code:\n\n{context}' -c 16384 code_folder -v --stats > ca_analysis.txt 2>ca_debug.txt


import os
import sys
import argparse
import openai
import tiktoken
from gitignore_parser import parse_gitignore
import time
import json
from typing import List, Optional

def read_file_in_chunks(file_path: str, chunk_size: int):
    """Read a file in chunks of specified size."""
    with open(file_path, 'r') as file:
        while True:
            lines = []
            for _ in range(chunk_size):
                line = file.readline()
                if not line:
                    break
                lines.append(line)
            if not lines:
                break
            yield ''.join(lines).strip()

def is_file_ignored_by_gitignore(file_path: str, gitignore_matchers: List) -> bool:
    """Check if a file is ignored by any .gitignore matcher."""
    return any(matcher(file_path) for matcher in gitignore_matchers)

def load_gitignore_files(directory: str) -> List:
    """Load .gitignore files from the directory and its parents."""
    gitignore_matchers = []
    current_dir = directory
    while current_dir != os.path.dirname(current_dir):  # Stop at the root directory
        gitignore_path = os.path.join(current_dir, '.gitignore')
        if os.path.exists(gitignore_path):
            gitignore_matchers.append(parse_gitignore(gitignore_path))
        current_dir = os.path.dirname(current_dir)

    # Check the current working directory's .gitignore
    cwd_gitignore_path = os.path.join(os.getcwd(), '.gitignore')
    if os.path.exists(cwd_gitignore_path):
        gitignore_matchers.append(parse_gitignore(cwd_gitignore_path))

    return gitignore_matchers

def call_openai_api(client, model: str, prompt: str, system_message: Optional[str] = None, verbose: bool = False):
    """Call the OpenAI API with the given parameters."""
    messages = []
    if system_message:
        messages.append({"role": "system", "content": system_message})
    messages.append({"role": "user", "content": prompt})

    if verbose:
        print(f"Raw request:\nModel: {model}\nMessages: {json.dumps(messages, indent=2)}\n", file=sys.stderr)

    start_time = time.time()
    response = client.chat.completions.create(
        model=model,
        messages=messages
    )
    elapsed_time = time.time() - start_time

    if verbose:
        print(f"Raw response:\n\t{str(response)}\n----------------\n\n", file=sys.stderr)

    return response.choices[0].message.content, elapsed_time

def count_tokens(text: str, encoder) -> int:
    """Count the number of tokens in the given text using the specified encoder."""
    return len(encoder.encode(text))

def process_files(directory: str, prompt: str, summary_prompt: Optional[str], context_length: int, model: str, system_message: Optional[str], file_filter: Optional[str], stats: bool, verbose: bool, extensions: Optional[List[str]] = None):
    """Process files in the directory with the given parameters."""
    print(f"directory is {directory}", file=sys.stderr)
    print(f"prompt is {prompt}", file=sys.stderr)
    print(f"summary_prompt is {summary_prompt}", file=sys.stderr)
    print(f"context_length is {context_length}", file=sys.stderr)
    print(f"model is {model}", file=sys.stderr)
    print(f"system_message is {system_message}", file=sys.stderr)
    print(f"file_filter is {file_filter}", file=sys.stderr)
    print(f"stats is {stats}", file=sys.stderr)
    print(f"verbose is {verbose}", file=sys.stderr)
    print(f"extensions is {extensions}", file=sys.stderr)
    
    encoder = tiktoken.encoding_for_model(model)
    total_responses = []
    total_input_tokens = 0
    total_output_tokens = 0
    total_api_time = 0

    client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))

    gitignore_matchers = load_gitignore_files(directory)

    if extensions is None:
        extensions = ['.py', '.dockerfile', '.sh', '.js']

    for root, _, files in os.walk(directory):
        for file in files:
            if any(file.endswith(ext) for ext in extensions):
                if file_filter and file_filter not in os.path.join(root, file):
                    continue
                file_path = os.path.join(root, file)
                if not is_file_ignored_by_gitignore(file_path, gitignore_matchers):
                    start_line = 1
                    chunk = ""
                    for line in read_file_in_chunks(file_path, 100):  # Read in chunks of 100 lines
                        chunk += line + "\n"
                        token_count = count_tokens(chunk, encoder)
                        if verbose:
                            print(f"Processing {file_path}, start_line {start_line}, token_count {token_count}", file=sys.stderr)
                        if token_count > context_length:
                            # Find the last whitespace before the context length boundary
                            split_point = chunk[:context_length].rfind(' ')
                            if split_point == -1:
                                split_point = context_length
                            chunk_to_send = chunk[:split_point]
                            remaining = chunk[split_point:].strip()
                            response, elapsed_time = call_openai_api(client, model, prompt.format(filename=file_path, startline=start_line, context=chunk_to_send), system_message, verbose)
                            total_responses.append(response)
                            total_input_tokens += count_tokens(chunk_to_send, encoder)
                            total_output_tokens += count_tokens(response, encoder)
                            total_api_time += elapsed_time
                            print(response)
                            start_line += chunk_to_send.count('\n')
                            chunk = remaining
                    if chunk:  # Process any remaining chunk
                        response, elapsed_time = call_openai_api(client, model, prompt.format(filename=file_path, startline=start_line, context=chunk), system_message, verbose)
                        total_responses.append(response)
                        total_input_tokens += count_tokens(chunk, encoder)
                        total_output_tokens += count_tokens(response, encoder)
                        total_api_time += elapsed_time
                        print(response)

    if summary_prompt:
        final_response, elapsed_time = call_openai_api(client, model, summary_prompt.format(context=' '.join(total_responses)), system_message, verbose)
        total_api_time += elapsed_time
        print(final_response)
        total_output_tokens += count_tokens(final_response, encoder)

    if stats:
        print("\n--- Stats ---", file=sys.stderr)
        print(f"Total execution time (API calls): {total_api_time:.2f} seconds", file=sys.stderr)
        print(f"Total input tokens: {total_input_tokens}", file=sys.stderr)
        print(f"Total output tokens: {total_output_tokens}", file=sys.stderr)
        print(f"Input tokens/sec: {total_input_tokens / total_api_time:.2f}", file=sys.stderr)
        print(f"Output tokens/sec: {total_output_tokens / total_api_time:.2f}", file=sys.stderr)

def main():
    parser = argparse.ArgumentParser(description="Process files with OpenAI-compatible API.")
    parser.add_argument('directory', help='Directory to process')
    parser.add_argument('-p', '--prompt', required=True, help='User prompt')
    parser.add_argument('-s', '--summary', help='Summary prompt')
    parser.add_argument('-c', '--context', type=int, required=True, help='Context length in tokens')
    parser.add_argument('-m', '--model', default='gpt-4o', help='Model name')
    parser.add_argument('--system', help='System message')
    parser.add_argument('-f', '--filter', help='Filter files by string in path')
    parser.add_argument('--stats', action='store_true', help='Print statistics')
    parser.add_argument('-v', '--verbose', action='store_true', help='Print raw request and response to stderr')
    parser.add_argument('-e', '--extensions', help='Comma-separated list of file extensions to process')

    args = parser.parse_args()
    print(f"args is {args}", file=sys.stderr)

    extensions = args.extensions.split(',') if args.extensions else None

    process_files(
        directory=args.directory,
        prompt=args.prompt,
        summary_prompt=args.summary,
        context_length=args.context,
        model=args.model,
        system_message=args.system,
        file_filter=args.filter,
        stats=args.stats,
        verbose=args.verbose,
        extensions=extensions
    )

if __name__ == "__main__":
    main()
